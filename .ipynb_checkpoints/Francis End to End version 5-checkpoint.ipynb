{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a74b426-5d2c-476f-a538-24260be719bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Details QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b96ce50-8a39-481f-b724-52e92f462111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import create_tagging_chain_pydantic\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36bbe172-3ce5-4ae1-b9d9-d568b1560a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a00abee-9260-49d8-a2e8-d08ee7aac156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access environment variables\n",
    "api_key = os.getenv(\"OPENAI_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2e0a59c-87f5-4808-b9c1-f6b430cad781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\", openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed0aa117",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"raw_data/one_day_test.csv\")\n",
    "all_available_countries = df['visited_countries'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff3a15c-bd79-4097-843a-cd1172ea2853",
   "metadata": {},
   "source": [
    "# stage analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f994d",
   "metadata": {},
   "source": [
    "## interest extraction and sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61bdd330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class user_interests(BaseModel):\n",
    "    interest: Optional[str] = Field(\n",
    "        None,\n",
    "        description=\"Any experiances, interests, places or activities the user has expressed an interest in. Leave blank if none are mentioned\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd029553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarise the found itineraries using map-reduce and then reduce the resulting summaries to rank \n",
    "# how well they relate to the user interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8454f403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map prompt\n",
    "def map_prompt(interests):\n",
    "    interests = \", \".join(interests)\n",
    "\n",
    "    intermediate_template = f\"\"\"Based on this list of docs, please identify the itinerary name, tour length, travel style, physical grading, an summary of the itinerary and this list of interests: {interests}.\n",
    "    If one or more of the interests is not mentioned this include this in the answer.\n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "    map_template = \"\"\"The following is a set of documents\n",
    "    {docs}\n",
    "    \"\"\"\n",
    "    map_template += intermediate_template\n",
    "\n",
    "    map_prompt = PromptTemplate.from_template(map_template)\n",
    "    return map_prompt\n",
    "\n",
    "# first reduce prompt\n",
    "def first_reduce_prompt():\n",
    "    # Reduce\n",
    "    reduce_template = \"\"\"The following is set of summaries:\n",
    "    {doc_summaries}\n",
    "    Take these and distill it into a final, consolidated summary of the main themes. \n",
    "    Helpful Answer:\"\"\"\n",
    "    reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "    return reduce_prompt\n",
    "\n",
    "# second reduce prompt\n",
    "def second_reduce_prompt(interests):\n",
    "    interests = \", \".join(interests)\n",
    "\n",
    "    intermediate_template = f\"\"\"Take these and rank them based on the following user interests: {interests}.\n",
    "    Return the origional summary along with a reason for ranking each itinerary based on their interests.\n",
    "    Refer to the user as - you.\n",
    "    Helpful Answer:\"\"\"\n",
    "    \n",
    "    reduce_template = \"\"\"\n",
    "    You are an AI travel agent speaking to a user. The following is set of summaries that fits the users basic travel needs:\n",
    "    {doc_summaries}\n",
    "    \"\"\"\n",
    "    reduce_template += intermediate_template\n",
    "    \n",
    "    reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "    return reduce_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99bb1e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution_presentation_prompt(found_itineraries, interests):\n",
    "    # Map\n",
    "    map_chain = LLMChain(llm=llm, prompt=map_prompt(interests))\n",
    "\n",
    "    # Run chain\n",
    "    reduce_chain = LLMChain(llm=llm, prompt=first_reduce_prompt())\n",
    "\n",
    "    # Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "    combine_documents_chain = StuffDocumentsChain(\n",
    "        llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    "    )\n",
    "\n",
    "    # Combines and iteravely reduces the mapped documents\n",
    "    reduce_documents_chain = ReduceDocumentsChain(\n",
    "        # This is final chain that is called.\n",
    "        combine_documents_chain=combine_documents_chain,\n",
    "        # If documents exceed context for `StuffDocumentsChain`\n",
    "        collapse_documents_chain=combine_documents_chain,\n",
    "        # The maximum number of tokens to group documents into.\n",
    "        token_max=4000,\n",
    "    )\n",
    "\n",
    "    # Combining documents by mapping a chain over them, then combining results\n",
    "    map_reduce_chain = MapReduceDocumentsChain(\n",
    "        # Map chain\n",
    "        llm_chain=map_chain,\n",
    "        # Reduce chain\n",
    "        reduce_documents_chain=reduce_documents_chain, #_documents\n",
    "        # The variable name in the llm_chain to put the documents in\n",
    "        document_variable_name=\"docs\",\n",
    "        # Return the results of the map steps in the output\n",
    "        return_intermediate_steps=False,\n",
    "    )\n",
    "\n",
    "    # list initial itinerary summaries\n",
    "    text_vars = []\n",
    "\n",
    "    for itinerary in found_itineraries[:]:\n",
    "        itinerary_path = f\"raw_data/itinerary_text/{itinerary}.txt\"\n",
    "        loader = TextLoader(itinerary_path)\n",
    "        docs = loader.load()\n",
    "        itinerary_summary = map_reduce_chain.run(docs)\n",
    "        itinerary_document = Document(page_content=itinerary_summary)\n",
    "        text_vars.append(itinerary_document)\n",
    "        print(itinerary_summary)\n",
    "\n",
    "    # Reduce\n",
    "    # final Reduce chain\n",
    "    reduce_chain = LLMChain(llm=llm, prompt=second_reduce_prompt(interests))\n",
    "\n",
    "    # Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "    combine_documents_chain = StuffDocumentsChain(\n",
    "        llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    "    )\n",
    "\n",
    "    # Combines and iteravely reduces the mapped documents\n",
    "    reduce_documents_chain = ReduceDocumentsChain(\n",
    "        # This is final chain that is called.\n",
    "        combine_documents_chain=combine_documents_chain,\n",
    "        # If documents exceed context for `StuffDocumentsChain`\n",
    "        collapse_documents_chain=combine_documents_chain,\n",
    "        # The maximum number of tokens to group documents into.\n",
    "        token_max=4000,\n",
    "    )\n",
    "\n",
    "\n",
    "    reduced = reduce_documents_chain.run(text_vars)\n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9688c348",
   "metadata": {},
   "source": [
    "## final stage analyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff197d79-e492-4555-8613-da68ab2e9b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TravelDetails(BaseModel):\n",
    "    introduction: Optional[bool] = Field(\n",
    "        False,\n",
    "        description=\"Has francis introducted himself and asked if the user is interested in a group tour.\",\n",
    "    )\n",
    "    qualification: Optional[str] = Field(\n",
    "        ...,\n",
    "        description=\"Did the user confirm they are looking for a group tour or answer positivley when asked. If the user asks about a trip assume the answer is yes\",\n",
    "        enum=[\"Yes\", \"No\", \"Unsure\"]\n",
    "    )\n",
    "    country: Optional[str] = Field(\n",
    "        \"\",\n",
    "        description=\"This is the name of the country the user is wanting to visit. If they name a place within a country always return the country\",\n",
    "        enum=[\"Cambodia\", \"Vietnam\", \"Morocco\", \"USA\"]\n",
    "    )\n",
    "    \n",
    "    departing_after: Optional[str] = Field(\n",
    "        \"\",\n",
    "        description=\"This is the first date from which the user can depart. If the user gives a month assume this is the first of the month. If not year if given return 2023. In the format '%Y-%m-%d'\",\n",
    "    )\n",
    "    departing_before: Optional[str] = Field(\n",
    "        \"\",\n",
    "        description=\"This is the last date from which the user can depart. If the user gives a month assume this is the last day of the month. If not year if given return 2023. In the format '%Y-%m-%d'\",\n",
    "    )\n",
    "    max_budget: Optional[int] = Field(\n",
    "        0,\n",
    "        description=\"This is the maximun amount of money the user is looking to spend on their trip.\",\n",
    "    )\n",
    "    max_duration: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"This is the maximum duration of their trip.\"\n",
    "    )\n",
    "    min_duration: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"This is the minimum duration of their trip.\",\n",
    "    )\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69de78ba-1c58-433f-864c-03653cf31487",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_for_dict = {\"country\":\"what country are you looking to travel to?\",\n",
    "                \"max_budget\":\"how much are you looking to spend?\",\n",
    "                \"departing_after\":\"when are you looking to travel?\",\n",
    "                \"departing_before\":\"when are you looking to travel?\",\n",
    "                \"max_duration\":\"how long do you want your trip to be?\",\n",
    "                \"min_duration\":\"how long do you want your trip to be?\"\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d877fa78-435b-4627-a4f4-bbc916b006c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_what_is_empty(user_travel_details):\n",
    "    ask_for = []\n",
    "    # Check if fields are empty\n",
    "    for field, value in user_travel_details.dict().items():\n",
    "        if value in [None, \"\", 0]:  # You can add other 'empty' conditions as per your requirements\n",
    "            ask_for.append(ask_for_dict[field])\n",
    "    \n",
    "    if 'what country are you looking to travel to?' in ask_for:\n",
    "        ask_for.remove('what country are you looking to travel to?')\n",
    "        ask_for.insert(0, 'what country are you looking to travel to?')\n",
    "    elif 'how much are you looking to spend?' in ask_for:\n",
    "        ask_for.remove('how much are you looking to spend?')\n",
    "        ask_for.insert(0, 'how much are you looking to spend?')\n",
    "\n",
    "    \n",
    "    return ask_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b4a599e-ebe3-4c75-a65f-adbb83cd4862",
   "metadata": {},
   "outputs": [],
   "source": [
    "## checking the response and adding it\n",
    "def add_non_empty_details(current_details: TravelDetails, new_details: TravelDetails):\n",
    "    non_empty_details = {k: v for k, v in new_details.dict().items() if v not in [False, None, \"\"]}\n",
    "    updated_details = current_details.copy(update=non_empty_details)\n",
    "    return updated_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88a27b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to find the first non-null value in columns 9 to the end of the df \n",
    "# used for finding the costs\n",
    "def find_first_non_null(row):\n",
    "    for value in row[12:]:  # Slice from the 9th column to the end\n",
    "        if not pd.isna(value):\n",
    "            return value\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a717df4f-854e-449e-aaec-0989caef0f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filtered_df(df, user_travel_details):\n",
    "    trip_details_dict = user_travel_details.dict()\n",
    "    filled_out_dictionary = {k: v for k, v in user_travel_details.dict().items() if v not in [False, None, \"\",0]}\n",
    "\n",
    "    # convert dates to datetime format\n",
    "    df['duration'] = df['duration'].str.replace(' days', '').astype(int)\n",
    "    df['start_date'] = pd.to_datetime(df['start_date'], format='%Y-%m-%d')\n",
    "    \n",
    "    # Apply the custom function to each row to find cost\n",
    "    df['first_non_null'] = df.apply(find_first_non_null, axis=1)\n",
    "\n",
    "    # Filtering the DataFrame\n",
    "    filtered_df = df.copy()  # Make a copy of the original DataFrame to keep it intact\n",
    "    \n",
    "    # Iterate through the list of potential inputs\n",
    "    for input_column in filled_out_dictionary.keys():\n",
    "        if input_column == 'country':\n",
    "            filtered_df = filtered_df[filtered_df['visited_countries'] == trip_details_dict[\"country\"]]\n",
    "        elif input_column == 'max_budget':\n",
    "            filtered_df = filtered_df[filtered_df['first_non_null'] <= trip_details_dict[\"max_budget\"]]\n",
    "        elif input_column == 'min_budget':\n",
    "            filtered_df = filtered_df[filtered_df['first_non_null'] >= trip_details_dict[\"min_budget\"]]\n",
    "        elif input_column == 'departing_after':\n",
    "            filtered_df = filtered_df[filtered_df['start_date'] >= trip_details_dict[\"departing_after\"]]\n",
    "        elif input_column == 'departing_before':\n",
    "            filtered_df = filtered_df[filtered_df['start_date'] <= trip_details_dict[\"departing_before\"]]\n",
    "        elif input_column == 'max_duration':\n",
    "            filtered_df = filtered_df[filtered_df['duration'] <= trip_details_dict[\"max_duration\"]]\n",
    "        elif input_column == 'min_duration':\n",
    "            filtered_df = filtered_df[filtered_df['duration'] >= trip_details_dict[\"min_duration\"]]\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebb7794c-9d35-4641-942b-a93cbf292799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItineraryDetails(BaseModel):\n",
    "    itinerary_name: Optional[str] = Field(\n",
    "        ...,\n",
    "        description=\"The name of the itinerary the user has decided to take not the country\",\n",
    "    )\n",
    "    itinerary_start_date: Optional[str] = Field(\n",
    "        ...,\n",
    "        description=\"The start date of the itinerary the user has decided to take. This is an exact date\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b9b11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function that can be used in the info gathering prompt\n",
    "# still being worked on to present a more conversational approach to gathering info\n",
    "# it can also be used to guide the user\n",
    "def prompt_amendments(ask_for, filtered_df, found_itineraries):\n",
    "    if ask_for[0] == 'how much are you looking to spend?':\n",
    "        min_budget = filtered_df['first_non_null'].min()\n",
    "        mean_budget = filtered_df['first_non_null'].mean()\n",
    "        return f'In your answer tell the user there are {len(found_itineraries)} itineraries that fit their needs.\\\n",
    "        The minimim trip cost is {min_budget} and the average tripcost is {mean_budget} to their destination.'\n",
    "    if ask_for[0] == 'when are you looking to travel?':\n",
    "         print('The question is about when to travel')\n",
    "    if ask_for[0] == 'how long do you want your trip to be?':\n",
    "         print('The question is about the duration')\n",
    "    if ask_for[0] == 'what country are you looking to travel to?':\n",
    "         print('The question is about where they want to travel to')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5102c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdb5f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualification stage prompts to make sure the user is interested in a group tour \n",
    "\n",
    "# Info gathering stage when there are still trip details to ask for\n",
    "def info_gathering_prompt(ask_for, found_itineraries, filtered_df):\n",
    "    if len(ask_for) == 6:\n",
    "        PROMPT_TEMPLATE = f\"\"\"You are currently in the detail gathering phase of the conversation and are trying to get detail of the users trip to help find the the perfect trip. \n",
    "            If the user has just asked a follow up question in the conversation history, answer it.\n",
    "            Once you have answered their question ALWAYS ask the user the following question to gather the required information.\n",
    "            Follow up question:\n",
    "            {ask_for[0]}\"\"\"\n",
    "    else:\n",
    "        PROMPT_TEMPLATE = f\"\"\"You are currently in the trip detail phase of the conversation and are trying to get detail of the users trip to help find the the perfect trip. \n",
    "            If the user has just asked a follow up question in the conversation history, answer it.\n",
    "            Once you have answered their question ALWAYS ask the user the following question to gather the required information.\n",
    "            Follow up question:\n",
    "            {ask_for[0]}\n",
    "            \"\"\"\n",
    "        # {prompt_amendments(ask_for, filtered_df, found_itineraries)} \n",
    "    return PROMPT_TEMPLATE\n",
    "\n",
    "# Info gathering stage when there are still trip details to ask for\n",
    "def interest_gathering_prompt(found_itineraries, new_user_travel_details, list_of_interests):\n",
    "    if len(list_of_interests) == 0:\n",
    "        PROMPT_TEMPLATE = f\"\"\"You have gathered all the basic information you need from the user: \n",
    "        - destination: {user_travel_details.dict()['country']}\n",
    "        - budget\n",
    "        - when they're looking to travel\n",
    "        - How long they want to travel for\n",
    "        You now need to learn more about the users interests to recommend to most suitable trip.\n",
    "        Ask questions around why they want to visit {user_travel_details.dict()['country']} \n",
    "        \"\"\"\n",
    "    else:\n",
    "        PROMPT_TEMPLATE = f\"\"\"You have gathered all the basic information you need from the user: \n",
    "        - destination: {user_travel_details.dict()['country']}\n",
    "        - budget\n",
    "        - when they're looking to travel\n",
    "        - How long they want to travel for\n",
    "        The following itineraries are available: {found_itineraries}\n",
    "        You now need to learn more about the users interests to recommend to most suitable trip.\n",
    "        This is a list of places & interests they have already expressed an interest in: {list_of_interests}\n",
    "        Ask questions around why they choose the destination \n",
    "        \"\"\"\n",
    "    return PROMPT_TEMPLATE\n",
    "    \n",
    "# still in the info gathering stage but there are no available itineraries based on preferences\n",
    "def no_results_prompt(user_travel_details, new_user_travel_details):\n",
    "    old_keys = new_user_travel_details.dict().keys()\n",
    "    new_keys = new_user_travel_details.dict().keys()\n",
    "    last_question = [x for x in new_keys if x not in old_keys]\n",
    "    conversation_stage = f\"\"\"\n",
    "        The user has provided details of their trip but unfortunatley there are no itineraries that match their needs. \n",
    "        Explain this to the user and higlight they need to look at alternate: {\", \".join(last_question)}\"\"\"\n",
    "    return conversation_stage\n",
    "\n",
    "# All the details are gathered and we're presenting a solution, list of available itineraries\n",
    "def solution_presentation_prompt(found_itineraries, df):\n",
    "    summary = \"\"\n",
    "    for itinerary in found_itineraries:\n",
    "        filtered_df = df[df['tour_name'] == itinerary]\n",
    "        summary += f'Itinerary: {filtered_df[\"tour_name\"].values[0]}\\n'\n",
    "        summary += f'Tour description: {filtered_df[\"tour_description\"].values[0]}\\n'\n",
    "        summary += f'Link: {filtered_df[\"url\"].values[0]}\\n\\n'\n",
    "    \n",
    "    conversation_stage = f\"\"\"Thank the user for providing the details. \n",
    "        Based on all the users needs here is the list of itineraries and a summary that fit their needs:\\n{summary} \n",
    "        Present the itinerary or itineraries to the user. \n",
    "        \"\"\"\n",
    "    # text summarisation needed for the above\n",
    "    return conversation_stage\n",
    "\n",
    "# follow up questions prompt about the trip and the company!\n",
    "# a prompt to answer questions about the tours once solutions have been presented\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "57b23452-7d55-4a00-8434-224cb58ca164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_conversation_stage(conversation_history_list, user_travel_details, user_interests, list_of_interests, interest_asked):\n",
    "\n",
    "    conversation_history = \"\\n\".join(conversation_history_list)\n",
    "\n",
    "    # extract travel details chain\n",
    "    chain = create_tagging_chain_pydantic(TravelDetails, llm)\n",
    "    res = chain.run(conversation_history)\n",
    "    new_user_travel_details = add_non_empty_details(user_travel_details, res)\n",
    "    ask_for = check_what_is_empty(new_user_travel_details)\n",
    "    \n",
    "    # extract intrests chain\n",
    "    chain = create_tagging_chain_pydantic(user_interests, llm)\n",
    "    interest = chain.run(conversation_history_list[-1])\n",
    "    if interest.dict()['interest'] != None:\n",
    "        list_of_interests.append(interest.dict()['interest'])\n",
    "\n",
    "    #load df\n",
    "    df = pd.read_csv(\"raw_data/one_day_test.csv\")\n",
    "    \n",
    "    # Filter df and gather available itineraries\n",
    "    filtered_df = get_filtered_df(df, new_user_travel_details)\n",
    "    found_itineraries = filtered_df['tour_name'].unique()\n",
    "    \n",
    "    # if there are no itineraries tha\n",
    "    if len(filtered_df) == 0:\n",
    "        conversation_stage = no_results_prompt(user_travel_details, new_user_travel_details)                        \n",
    "        return conversation_stage, new_user_travel_details\n",
    "\n",
    "    # if we have all the validated details we need to ask for a clients interests\n",
    "    elif len(ask_for) == 0 and len(found_itineraries) > 0 and len(interest_asked) == 0:\n",
    "        print(\"All details gathered! Ask about interests...\")\n",
    "        conversation_stage = interest_gathering_prompt(found_itineraries, new_user_travel_details, list_of_interests)\n",
    "        interest_asked.append(1)\n",
    "        return conversation_stage, new_user_travel_details\n",
    "\n",
    "    # if we have all the validated details we need to present the list of itineraries and have the client decide\n",
    "    elif len(ask_for) == 0 and len(found_itineraries) > 0:\n",
    "        print(\"All details gathered! summarise the itineraries...\")\n",
    "        conversation_stage = solution_presentation_prompt(found_itineraries, df)   \n",
    "        return conversation_stage, new_user_travel_details\n",
    "    \n",
    "    if user_travel_details.dict()['qualification'] == 'no':\n",
    "        conversation_stage = \"The user is not interested in a group tour so politely end the conversation. Ask them to come back if they ever are\"\n",
    "    elif user_travel_details.dict()['qualification'] == 'Unsure':\n",
    "        conversation_stage = \"Explain what a group tour is. End your response by asking the user if they are interested in a group tour?\"\n",
    "    else:\n",
    "        conversation_stage = info_gathering_prompt(ask_for, found_itineraries, filtered_df)\n",
    "        asked_for.append(ask_for[0])\n",
    "\n",
    "    return conversation_stage, new_user_travel_details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2a6846-d3fd-414a-a29d-b9c2546980e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b929bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumarization_tool(input=\"\"):\n",
    "    return 'a 15 day itinerary through morocco.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7f4ca2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import PythonAstREPLTool\n",
    "df = pd.read_csv(\"raw_data/one_day_test.csv\")\n",
    "\n",
    "class PythonInputs(BaseModel):\n",
    "    query: str = Field(description=\"code snippet to run\")\n",
    "        \n",
    "repl = PythonAstREPLTool(locals={\"df\": df}, \n",
    "                         name=\"multi_county_query\",\n",
    "                         description=\"useful for answering questions about multiple country costs e.g. average trip cost to peru\",\n",
    "                         args_schema=PythonInputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e854c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def get_tools():          \n",
    "    #tru start\n",
    "    class DocumentInput(BaseModel):\n",
    "        question: str = Field()\n",
    "\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\", openai_api_key=api_key)\n",
    "\n",
    "    tools = []\n",
    "    \n",
    "    files = []\n",
    "    for file in glob.glob('raw_data/itinerary_text/*'):\n",
    "        file_dict = {'name': file.split(\"/\")[-1].replace('.txt','').replace('& ','').replace(':','').replace(' ','_'),\n",
    "                    \"path\": file}\n",
    "        files.append(file_dict)\n",
    "\n",
    "    for file in files[:10]:\n",
    "        loader = TextLoader(file[\"path\"])\n",
    "        pages = loader.load_and_split()\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "        docs = text_splitter.split_documents(pages)\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "        retriever = FAISS.from_documents(docs, embeddings).as_retriever()\n",
    "\n",
    "        # Wrap retrievers in a Tool\n",
    "        tools.append(\n",
    "            Tool(\n",
    "                args_schema=DocumentInput,\n",
    "                name=file[\"name\"],\n",
    "                description=f\"needed when you want to answer questions about {file['name']} itinerary\",\n",
    "                func=RetrievalQA.from_chain_type(llm=llm, retriever=retriever),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "#     tools.append(PythonAstREPLTool(locals={\"df\": df}, \n",
    "#                          name=\"multi_county_query\",\n",
    "#                          description=\"useful for answering questions about multiple country costs e.g. average trip cost to peru\",\n",
    "#                          args_schema=PythonInputs))\n",
    "        \n",
    "    return tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb63c7-562a-49d5-aa52-f1d0de09915d",
   "metadata": {},
   "source": [
    "# Sales Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f340dfd2-c7db-439c-9afa-aaa341b25c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import OpenAIFunctionsAgent, AgentExecutor\n",
    "from langchain.agents import Tool\n",
    "from langchain.schema.messages import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f5a47031-84b8-421a-a564-93e196bfb5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SALES_AGENT_TOOLS_PROMPT = \"\"\"\n",
    "Never forget your name is Francis. You work as a Travel Agent.\n",
    "You work at company named Francis. Francis's business is the following: Francis is a context aware AI travel agent that works on finding users their dream group tour holiday.\n",
    "You are contacting a potential prospect in order to find them a group toup holiday.\n",
    "Your means of contacting the prospect is live chat.\n",
    "\n",
    "Keep your responses in short length to retain the user's attention. Never produce lists, just answers.\n",
    "\n",
    "Always think about the following conversation stage you are at before answering:\n",
    "\n",
    " - {conversation_stage}\n",
    "\n",
    "You MUST respond according to the previous conversation history and the stage of the conversation you are at.\n",
    "If you get asked about an itinerary use the tools available to you, do not make up an answer. If you do not know the answer tell the user.\n",
    "Only generate one response at a time and act as Francis only!\n",
    "Do not add Francis: to your output.\n",
    "\n",
    "Previous conversation history:\n",
    "{conversation_history}\n",
    "\n",
    "Begin!\n",
    "\"\"\"\n",
    "\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "66d6c897-0d66-40f8-b23d-23f48643bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customize_prompt(conversation_history, conversation_stage, SALES_AGENT_TOOLS_PROMPT):\n",
    "\n",
    "    conversation_history = \"\\n\".join(conversation_history)\n",
    "    \n",
    "    \n",
    "    from langchain import LLMChain, PromptTemplate\n",
    "    prompt = PromptTemplate(\n",
    "                template=SALES_AGENT_TOOLS_PROMPT,\n",
    "                input_variables=[\n",
    "                    \"conversation_stage\",\n",
    "                    \"conversation_history\"\n",
    "                ],\n",
    "            )\n",
    "    \n",
    "    SALES_AGENT_TOOLS_PROMPT = prompt.format(conversation_stage=conversation_stage, \n",
    "                                     conversation_history=conversation_history)\n",
    "    \n",
    "    system_message = SystemMessage(\n",
    "            content=(SALES_AGENT_TOOLS_PROMPT\n",
    "            )\n",
    "    )\n",
    "    \n",
    "    prompt = OpenAIFunctionsAgent.create_prompt(\n",
    "            system_message=system_message\n",
    "    )\n",
    "\n",
    "    return prompt\n",
    "# print(prompt.messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "928fde58-12ae-49ff-a833-51645d0987bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 759, which is longer than the specified 500\n",
      "Created a chunk of size 917, which is longer than the specified 500\n",
      "Created a chunk of size 830, which is longer than the specified 500\n",
      "Created a chunk of size 771, which is longer than the specified 500\n",
      "Created a chunk of size 559, which is longer than the specified 500\n",
      "Created a chunk of size 800, which is longer than the specified 500\n",
      "Created a chunk of size 710, which is longer than the specified 500\n",
      "Created a chunk of size 724, which is longer than the specified 500\n",
      "Created a chunk of size 902, which is longer than the specified 500\n",
      "Created a chunk of size 759, which is longer than the specified 500\n",
      "Created a chunk of size 762, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "# Define the tools\n",
    "tools = get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d9be55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import OpenAIMultiFunctionsAgent\n",
    "def run_francis(input, conversation_history, user_travel_details, list_of_interests, interest_asked):\n",
    "    llm = ChatOpenAI(temperature=0.9, model=\"gpt-3.5-turbo\", openai_api_key=api_key)\n",
    "\n",
    "    user_input = f\"User: {input}\"\n",
    "\n",
    "    conversation_history.append(user_input)\n",
    "\n",
    "    conversation_stage, user_travel_details = check_conversation_stage(conversation_history, \n",
    "                                                                       user_travel_details,\n",
    "                                                                       user_interests, \n",
    "                                                                       list_of_interests, interest_asked)\n",
    "    \n",
    "    final_prompt = customize_prompt(conversation_history, conversation_stage, SALES_AGENT_TOOLS_PROMPT)\n",
    "    \n",
    "    # Create the agent\n",
    "    agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=final_prompt)\n",
    "    # agent = OpenAIMultiFunctionsAgent(llm=llm, tools=tools, prompt=final_prompt)\n",
    "    \n",
    "    # Run the agent with the actions\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False, max_iterations=5)\n",
    "    \n",
    "    francis = agent_executor.run(input)\n",
    "    francis1 = f\"Francis: {francis}\"\n",
    "    conversation_history.append(francis1)\n",
    "\n",
    "    return francis, user_travel_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e18bc7fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ad\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ad' is not defined"
     ]
    }
   ],
   "source": [
    "ad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9e0249-c67e-4c2e-8255-21d5194e5a77",
   "metadata": {},
   "source": [
    "# conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d04585d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d7c880d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_travel_details = TravelDetails(introduction=False,\n",
    "                                qualification=\"\",\n",
    "                                country=\"\",\n",
    "                                departing_after=None,\n",
    "                                departing_before=None,\n",
    "                                max_budget=None,\n",
    "                                max_duration=None,\n",
    "                                min_duration=None,\n",
    "                                # places_of_interest=\"\"\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3cc7f8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_interests = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0ebd1d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_asked = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0021a233-91d4-42c4-9b4c-af45f336c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = [\"Francis:  Hello, this is Francis from Francis Travel. Can i help you find a group tour today?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5281ee56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! What country are you looking to travel to?\n"
     ]
    }
   ],
   "source": [
    "human_input = \"yes\"\n",
    "francis_output, user_travel_details = run_francis(human_input, conversation_history, user_travel_details, list_of_interests, interest_asked)\n",
    "print(francis_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "33c348cf-fa3f-4f67-9c24-11695dacfdf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great choice! Morocco is a beautiful country with a rich culture and history. How much are you looking to spend on your trip to Morocco?\n"
     ]
    }
   ],
   "source": [
    "human_input = \"I want to go to Morocco\"\n",
    "francis_output, user_travel_details = run_francis(human_input, conversation_history, user_travel_details, list_of_interests, interest_asked)\n",
    "print(francis_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1111ba04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! With a budget of £2000, we can find you some amazing group tour options in Morocco. When are you looking to travel?\n"
     ]
    }
   ],
   "source": [
    "human_input = \"£2000\"\n",
    "francis_output, user_travel_details = run_francis(human_input, conversation_history, user_travel_details, list_of_interests, interest_asked)\n",
    "print(francis_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "926c5ae0-442e-4b92-8e37-61aac6694190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 867ab42db3d43653f550660dfb4f3a6b in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 867ab42db3d43653f550660dfb4f3a6b in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 867ab42db3d43653f550660dfb4f3a6b in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Fri, 06 Oct 2023 19:56:09 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-0613', 'openai-organization': 'user-bh4migofvvcva9c8ckdh0jm9', 'openai-processing-ms': '2342', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '89497', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '334ms', 'x-request-id': '867ab42db3d43653f550660dfb4f3a6b', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '8120773caef7496e-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! April 2024 sounds like a wonderful time to visit Morocco. How long do you want your trip to be?\n"
     ]
    }
   ],
   "source": [
    "human_input = \"april 2024\"\n",
    "francis_output, user_travel_details = run_francis(human_input, conversation_history, user_travel_details, list_of_interests, interest_asked)\n",
    "print(francis_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4e68eb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All details gathered! Ask about interests...\n",
      "Great! With a trip duration of between 4 and 30 days, we have several options for you in Morocco. Are you looking for a shorter or longer trip?\n"
     ]
    }
   ],
   "source": [
    "human_input = \"between 4 and 30 days\"\n",
    "francis_output, user_travel_details = run_francis(human_input, conversation_history, user_travel_details, list_of_interests, interest_asked)\n",
    "print(francis_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c150a427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All details gathered! summarise the itineraries...\n",
      "Great! Surfing in Morocco can be an amazing experience. I have a group tour option that includes surfing in Morocco. It's called \"Coastal Morocco: Waves & Market Stalls\". Here is a summary of the tour:\n",
      "\n",
      "Tour description: Now's the time to explore Morocco on your terms with this tour that's affordable for young travellers. Fly high on rooftop bars, and get your shopping on in this whirlwind five-day adventure along the coast of Africa's northernmost country. Your journey in Morocco includes stops in the village of Izourki Oufella, known for its refined honey and argan oil production, and the beaches of Essaouira. Enjoy free time to grab a tan or get your feet (and, let's be honest, the rest of you) wet with a surfing lesson. Short and sweet, this trip is perfect for kicking back or stepping on the gas.\n",
      "\n",
      "You can find more information about the tour and book it here: [Coastal Morocco: Waves & Market Stalls](https://www.gadventures.com/trips/tour-coastal-morocco/DCKE/)\n"
     ]
    }
   ],
   "source": [
    "human_input = \"I want to go surfing\"\n",
    "francis_output, user_travel_details = run_francis(human_input, conversation_history, user_travel_details, list_of_interests, interest_asked)\n",
    "print(francis_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "83770f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Francis:  Hello, this is Francis from Francis Travel. Can i help you find a group tour today?',\n",
       " 'User: yes',\n",
       " 'Francis: Great! What country are you looking to travel to?',\n",
       " 'User: I want to go to Morocco',\n",
       " 'Francis: Great choice! Morocco is a beautiful country with a rich culture and history. How much are you looking to spend on your trip to Morocco?',\n",
       " 'User: £2000',\n",
       " 'Francis: Great! With a budget of £2000, we can find you some amazing group tour options in Morocco. When are you looking to travel?',\n",
       " 'User: april 2024',\n",
       " 'Francis: Great! April 2024 sounds like a wonderful time to visit Morocco. How long do you want your trip to be?',\n",
       " 'User: between 4 and 30 days',\n",
       " 'Francis: Great! With a trip duration of between 4 and 30 days, we have several options for you in Morocco. Are you looking for a shorter or longer trip?',\n",
       " 'User: I want to go surfing',\n",
       " 'Francis: Great! Surfing in Morocco can be an amazing experience. I have a group tour option that includes surfing in Morocco. It\\'s called \"Coastal Morocco: Waves & Market Stalls\". Here is a summary of the tour:\\n\\nTour description: Now\\'s the time to explore Morocco on your terms with this tour that\\'s affordable for young travellers. Fly high on rooftop bars, and get your shopping on in this whirlwind five-day adventure along the coast of Africa\\'s northernmost country. Your journey in Morocco includes stops in the village of Izourki Oufella, known for its refined honey and argan oil production, and the beaches of Essaouira. Enjoy free time to grab a tan or get your feet (and, let\\'s be honest, the rest of you) wet with a surfing lesson. Short and sweet, this trip is perfect for kicking back or stepping on the gas.\\n\\nYou can find more information about the tour and book it here: [Coastal Morocco: Waves & Market Stalls](https://www.gadventures.com/trips/tour-coastal-morocco/DCKE/)']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fdb2ab65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All details gathered! summarise the itineraries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised APIError: The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 8d622d6a216ce6e5af45312fd36114e1 in your email.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 8d622d6a216ce6e5af45312fd36114e1 in your email.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID 8d622d6a216ce6e5af45312fd36114e1 in your email.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Fri, 06 Oct 2023 20:08:26 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-model': 'gpt-3.5-turbo-0613', 'openai-organization': 'user-bh4migofvvcva9c8ckdh0jm9', 'openai-processing-ms': '566', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3500', 'x-ratelimit-limit-tokens': '90000', 'x-ratelimit-remaining-requests': '3499', 'x-ratelimit-remaining-tokens': '88897', 'x-ratelimit-reset-requests': '17ms', 'x-ratelimit-reset-tokens': '735ms', 'x-request-id': '8d622d6a216ce6e5af45312fd36114e1', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '81208947d9c548af-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "'Coastal_Morocco_Waves_Market_Stalls_._dimensions' does not match '^[a-zA-Z0-9_-]{1,64}$' - 'messages.2.function_call.name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [83], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m human_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhow long is it?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m francis_output, user_travel_details \u001b[38;5;241m=\u001b[39m run_francis(human_input, conversation_history, user_travel_details, list_of_interests, interest_asked)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(francis_output)\n",
      "Cell \u001b[0;32mIn [66], line 23\u001b[0m, in \u001b[0;36mrun_francis\u001b[0;34m(input, conversation_history, user_travel_details, list_of_interests, interest_asked)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# agent = OpenAIMultiFunctionsAgent(llm=llm, tools=tools, prompt=final_prompt)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Run the agent with the actions\u001b[39;00m\n\u001b[1;32m     21\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m AgentExecutor(agent\u001b[38;5;241m=\u001b[39magent, tools\u001b[38;5;241m=\u001b[39mtools, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m francis \u001b[38;5;241m=\u001b[39m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m francis1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrancis: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfrancis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m conversation_history\u001b[38;5;241m.\u001b[39mappend(francis1)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/langchain/chains/base.py:475\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    476\u001b[0m         _output_key\n\u001b[1;32m    477\u001b[0m     ]\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    481\u001b[0m         _output_key\n\u001b[1;32m    482\u001b[0m     ]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/langchain/chains/base.py:282\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    283\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    284\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    285\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    286\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/langchain/chains/base.py:276\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[1;32m    270\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    271\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 276\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    281\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/langchain/agents/agent.py:1036\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1036\u001b[0m     next_step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_next_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname_to_tool_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolor_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return(\n\u001b[1;32m   1045\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m   1046\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/langchain/agents/agent.py:833\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    830\u001b[0m     intermediate_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_intermediate_steps(intermediate_steps)\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;66;03m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[0;32m--> 833\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mintermediate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OutputParserException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_parsing_errors, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/langchain/agents/openai_functions_agent/base.py:211\u001b[0m, in \u001b[0;36mOpenAIFunctionsAgent.plan\u001b[0;34m(self, intermediate_steps, callbacks, with_functions, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m messages \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mto_messages()\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_functions:\n\u001b[0;32m--> 211\u001b[0m     predicted_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m     predicted_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mpredict_messages(\n\u001b[1;32m    218\u001b[0m         messages,\n\u001b[1;32m    219\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    220\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/langchain/chat_models/base.py:601\u001b[0m, in \u001b[0;36mBaseChatModel.predict_messages\u001b[0;34m(self, messages, stop, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m     _stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(stop)\n\u001b[0;32m--> 601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/langchain/chat_models/base.py:551\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[0;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    546\u001b[0m     messages: List[BaseMessage],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    550\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m--> 551\u001b[0m     generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/langchain/chat_models/base.py:309\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    308\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 309\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    310\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    311\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    313\u001b[0m ]\n\u001b[1;32m    314\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/langchain/chat_models/base.py:299\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 299\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m         )\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/langchain/chat_models/base.py:446\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/langchain/chat_models/openai.py:339\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    338\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 339\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/langchain/chat_models/openai.py:278\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tenacity/__init__.py:326\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tenacity/__init__.py:406\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 406\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tenacity/__init__.py:351\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    349\u001b[0m is_explicit_retry \u001b[38;5;241m=\u001b[39m retry_state\u001b[38;5;241m.\u001b[39moutcome\u001b[38;5;241m.\u001b[39mfailed \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(retry_state\u001b[38;5;241m.\u001b[39moutcome\u001b[38;5;241m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry(retry_state\u001b[38;5;241m=\u001b[39mretry_state)):\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/tenacity/__init__.py:409\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    411\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/langchain/chat_models/openai.py:276\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/openai/api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[0;32m--> 298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/openai/api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    694\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    697\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 700\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/openai/api_requestor.py:763\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    764\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    765\u001b[0m     )\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: 'Coastal_Morocco_Waves_Market_Stalls_._dimensions' does not match '^[a-zA-Z0-9_-]{1,64}$' - 'messages.2.function_call.name'"
     ]
    }
   ],
   "source": [
    "human_input = \"how long is it?\"\n",
    "francis_output, user_travel_details = run_francis(human_input, conversation_history, user_travel_details, list_of_interests, interest_asked)\n",
    "print(francis_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db329b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_input = \"What is the day by day summary?\"\n",
    "francis_output, user_travel_details = run_francis(human_input, conversation_history, user_travel_details, user_itinerary_details)\n",
    "print(francis_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a6d087",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_input = \"Can you give me some more details?\"\n",
    "francis_output, user_travel_details = run_francis(human_input, conversation_history, user_travel_details, user_itinerary_details)\n",
    "print(francis_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd45d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
